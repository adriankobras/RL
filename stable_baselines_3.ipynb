{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 14.0\n",
      "Episode: 2, Score: 13.0\n",
      "Episode: 3, Score: 39.0\n",
      "Episode: 4, Score: 16.0\n",
      "Episode: 5, Score: 11.0\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "env = Monitor(env)\n",
    "\n",
    "# action space:         env.action_space\n",
    "# obersevation space:   env.observation_space\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # take random action\n",
    "        n_state, reward, done, trunc, info = env.step(action) # apply action to environment\n",
    "        score += reward\n",
    "    print(f'Episode: {episode}, Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0088098e-01, -2.2695049e+38,  3.9206123e-01, -6.4406351e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env]) # The algorithms require a vectorized environment to run with lambda as a function\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path) # MlpPolicy is a simple policy that only has one hidden layer with 64 neurons\n",
    "# Stable baselines 3 has three policy types: MlpPolicy, CnnPolicy, MultiInputPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_5\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 5392 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3302        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008760935 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.0113      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.39        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2878        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007893461 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0587      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 37.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2804        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011882458 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2744        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007172296 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 64          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2703        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008565776 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 67.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2657        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009263591 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 64.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2617         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058162333 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34           |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00715     |\n",
      "|    value_loss           | 45.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2607         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038862564 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    value_loss           | 49.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2584         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072742654 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.58        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 24           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1696b4e20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PPO??\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Save and reload model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPO model is considered solved for score 500\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: [300.]\n",
      "Episode: 2, Score: [331.]\n",
      "Episode: 3, Score: [500.]\n",
      "Episode: 4, Score: [500.]\n",
      "Episode: 5, Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs) # take action from model\n",
    "        obs, reward, done, info = env.step(action) # apply action to environment\n",
    "        score += reward\n",
    "    print(f'Episode: {episode}, Score: {score}')\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01614755,  0.23278338,  0.00951417, -0.33587608]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{'TimeLimit.truncated': False}])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action) # reward is 1 for every step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. View logs in Tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core metrics to look at:\n",
    "# 1. average reward\n",
    "# 2. avrage episode length\n",
    "\n",
    "# training strategies:\n",
    "# 1. train for longer\n",
    "# 2. hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Adding a callback to the training stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=10000, best_model_save_path=save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_6\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 5489     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.9        |\n",
      "|    ep_rew_mean          | 26.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3536        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009472036 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00179     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.4         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 50          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.2        |\n",
      "|    ep_rew_mean          | 33.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3110        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010105099 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 34.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 43.3        |\n",
      "|    ep_rew_mean          | 43.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2903        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008281752 |\n",
      "|    clip_fraction        | 0.0888      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=390.00 +/- 137.19\n",
      "Episode length: 390.00 +/- 137.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 390         |\n",
      "|    mean_reward          | 390         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008140159 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 56.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.6     |\n",
      "|    ep_rew_mean     | 55.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2638     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 68.6        |\n",
      "|    ep_rew_mean          | 68.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2632        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009573501 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 50.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 87           |\n",
      "|    ep_rew_mean          | 87           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2576         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050745136 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.492        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | 104          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2540         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063499776 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.557        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0108      |\n",
      "|    value_loss           | 53.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 122          |\n",
      "|    ep_rew_mean          | 122          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2526         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069758543 |\n",
      "|    clip_fraction        | 0.0597       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00928     |\n",
      "|    value_loss           | 59.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038154873 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.39         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00701     |\n",
      "|    value_loss           | 39.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x30f780e20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Change policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = dict(pi=[128,128,128,128], vf=[128,128,128,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, policy_kwargs={'net_arch':net_arch}, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_9\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 456      |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    fps             | 4182     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 478          |\n",
      "|    ep_rew_mean          | 478          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 498          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050710468 |\n",
      "|    clip_fraction        | 0.0916       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.716        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.42         |\n",
      "|    n_updates            | 95           |\n",
      "|    policy_gradient_loss | -0.00831     |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 485         |\n",
      "|    ep_rew_mean          | 485         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020576388 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 489          |\n",
      "|    ep_rew_mean          | 489          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 312          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125937145 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | 0.532        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.125        |\n",
      "|    n_updates            | 115          |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 0.746        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9520, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9520         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073642367 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.282        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0802       |\n",
      "|    n_updates            | 125          |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 0.57         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 491      |\n",
      "|    ep_rew_mean     | 491      |\n",
      "| time/              |          |\n",
      "|    fps             | 301      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 482          |\n",
      "|    ep_rew_mean          | 482          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 289          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027530584 |\n",
      "|    clip_fraction        | 0.0793       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | -0.0492      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0155       |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | 0.0023       |\n",
      "|    value_loss           | 0.365        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 485          |\n",
      "|    ep_rew_mean          | 485          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 276          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041419826 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | 0.0257       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 145          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 8.54         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 487         |\n",
      "|    ep_rew_mean          | 487         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005311896 |\n",
      "|    clip_fraction        | 0.0605      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | -0.0366     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 155         |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 488          |\n",
      "|    ep_rew_mean          | 488          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 264          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039044367 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.422       |\n",
      "|    explained_variance   | -0.0639      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00797      |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | -0.000294    |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19520, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19520        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044952836 |\n",
      "|    clip_fraction        | 0.0794       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.413       |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0445       |\n",
      "|    n_updates            | 175          |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 0.0874       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 489      |\n",
      "|    ep_rew_mean     | 489      |\n",
      "| time/              |          |\n",
      "|    fps             | 255      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17bd7e830>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Using an alternate algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
